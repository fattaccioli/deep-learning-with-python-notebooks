{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "This is a companion notebook for the book [Deep Learning with Python, Third Edition](TODO). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n\n**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install keras-nightly --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Two families of object detection models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "##### The R-CNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "##### Single-stage detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Object detection with a pretrained model: RetinaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "##### Introducing the dataset: Pascal VOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "\n",
    "!tar -xf VOCtrainval_06-Nov-2007.tar\n",
    "!tar -xf VOCtest_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as tf\n",
    "\n",
    "BASE_DIR = os.path.join(os.getcwd(), \"VOCdevkit\", \"VOC2007\")\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, \"JPEGImages\")\n",
    "ANNOTATION_DIR = os.path.join(BASE_DIR, \"Annotations\")\n",
    "IMAGESET_DIR = os.path.join(BASE_DIR, \"ImageSets\", \"Main\")\n",
    "CLASSES = {\n",
    "    0: \"aeroplane\",\n",
    "    1: \"bicycle\",\n",
    "    2: \"bird\",\n",
    "    3: \"boat\",\n",
    "    4: \"bottle\",\n",
    "    5: \"bus\",\n",
    "    6: \"car\",\n",
    "    7: \"cat\",\n",
    "    8: \"chair\",\n",
    "    9: \"cow\",\n",
    "    10: \"diningtable\",\n",
    "    11: \"dog\",\n",
    "    12: \"horse\",\n",
    "    13: \"motorbike\",\n",
    "    14: \"person\",\n",
    "    15: \"pottedplant\",\n",
    "    16: \"sheep\",\n",
    "    17: \"sofa\",\n",
    "    18: \"train\",\n",
    "    19: \"tvmonitor\",\n",
    "}\n",
    "\n",
    "def parse_annotation(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "\n",
    "    for obj in root.findall(\"object\"):\n",
    "        name = obj.find(\"name\").text\n",
    "        difficult = int(obj.find(\"difficult\").text)\n",
    "        if difficult:\n",
    "            continue\n",
    "\n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        size = root.find(\"size\")\n",
    "        width = float(size.find(\"width\").text)\n",
    "        height = float(size.find(\"height\").text)\n",
    "\n",
    "        xmin = float(bbox.find(\"xmin\").text) / width\n",
    "        ymin = float(bbox.find(\"ymin\").text) / height\n",
    "        xmax = float(bbox.find(\"xmax\").text) / width\n",
    "        ymax = float(bbox.find(\"ymax\").text) / height\n",
    "        bboxes.append([ymin, xmin, ymax, xmax])\n",
    "\n",
    "        class_idx = [k for k, v in CLASSES.items() if v == name][0]\n",
    "        labels.append(class_idx)\n",
    "    bboxes = tf.constant(bboxes, dtype=tf.float32)\n",
    "    labels = tf.constant(labels, dtype=tf.float32)\n",
    "    return bboxes, labels\n",
    "\n",
    "def process_example(image_id):\n",
    "    image_id = tf.compat.as_str_any(image_id.numpy())\n",
    "    image_path = os.path.join(IMAGE_DIR, f\"{image_id.rstrip()}.jpg\")\n",
    "    image_data = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(image_data, channels=3)\n",
    "    path = os.path.join(ANNOTATION_DIR, f\"{image_id.rstrip()}.xml\")\n",
    "    bboxes, labels = parse_annotation(path)\n",
    "    return image, bboxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def get_dataset(split, shuffle_files=True, shuffle_buffer_size=1000):\n",
    "    split_file = os.path.join(IMAGESET_DIR, f\"{split}.txt\")\n",
    "    with open(split_file, \"r\") as f:\n",
    "        image_ids = [x.strip() for x in f.readlines()]\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(image_ids)\n",
    "\n",
    "    if shuffle_files:\n",
    "        ds = ds.shuffle(shuffle_buffer_size)\n",
    "\n",
    "    ds = ds.map(\n",
    "        lambda x: tf.py_function(\n",
    "            func=process_example, inp=[x], Tout=[tf.uint8, tf.float32, tf.int64]\n",
    "        ),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "    ds = ds.map(\n",
    "        lambda image, bbox, label: {\n",
    "            \"image\": tf.ensure_shape(image, [None, None, 3]),\n",
    "            \"objects\": {\n",
    "                \"bbox\": tf.ensure_shape(bbox, [None, 4]),\n",
    "                \"label\": tf.ensure_shape(label, [None]),\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = get_dataset(\"trainval\", shuffle_files=True)\n",
    "eval_ds = get_dataset(\"test\", shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "example = next(iter(train_ds))\n",
    "\n",
    "plot_bounding_box_gallery(\n",
    "    np.array([example[\"image\"]]),\n",
    "    bounding_box_format=\"rel_yxyx\",\n",
    "    y_true={\n",
    "        \"boxes\": np.array([example[\"objects\"][\"bbox\"]]),\n",
    "        \"labels\": np.array([example[\"objects\"][\"label\"]]),\n",
    "    },\n",
    "    scale=8,\n",
    "    class_mapping=CLASSES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "##### Bounding box formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "##### Setting up an image preprocessing and augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "BBOX_FORMAT = \"yxyx\"\n",
    "\n",
    "def parse_record(record):\n",
    "    image = record[\"image\"]\n",
    "    h, w = tf.shape(image)[0], tf.shape(image)[1]\n",
    "    rel_boxes = record[\"objects\"][\"bbox\"]\n",
    "    abs_boxes = keras.utils.bounding_boxes.convert_format(\n",
    "        rel_boxes,\n",
    "        source=\"rel_yxyx\",\n",
    "        target=BBOX_FORMAT,\n",
    "        height=h,\n",
    "        width=w,\n",
    "    )\n",
    "    labels = tf.cast(record[\"objects\"][\"label\"], dtype=tf.int32)\n",
    "    return {\n",
    "        \"images\": image,\n",
    "        \"bounding_boxes\": {\n",
    "            \"boxes\": abs_boxes,\n",
    "            \"labels\": labels,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from keras.visualization import plot_bounding_box_gallery\n",
    "\n",
    "IMAGE_SIZE = (640, 640)\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "resizing = keras.layers.Resizing(\n",
    "    height=IMAGE_SIZE[0],\n",
    "    width=IMAGE_SIZE[1],\n",
    "    interpolation=\"bilinear\",\n",
    "    pad_to_aspect_ratio=True,\n",
    "    bounding_box_format=BBOX_FORMAT,\n",
    ")\n",
    "\n",
    "max_box_layer = keras.layers.MaxNumBoundingBoxes(\n",
    "    max_number=100,\n",
    "    bounding_box_format=BBOX_FORMAT,\n",
    ")\n",
    "\n",
    "data_augmentation_layers = [\n",
    "    keras.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=BBOX_FORMAT),\n",
    "]\n",
    "\n",
    "def prepare_dataset(ds, batch_size=4):\n",
    "    ds = ds.map(parse_record)\n",
    "    ds = ds.map(lambda x: resizing(x))\n",
    "    for layer in data_augmentation_layers:\n",
    "        ds = ds.map(lambda x: layer(x))\n",
    "    ds = ds.map(max_box_layer)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds_prepared = prepare_dataset(train_ds, batch_size=BATCH_SIZE)\n",
    "eval_ds_prepared = prepare_dataset(eval_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "first_images_unprepared = next(iter(train_ds.take(1)))\n",
    "\n",
    "plot_bounding_box_gallery(\n",
    "    np.array([first_images_unprepared[\"image\"]]),\n",
    "    bounding_box_format=\"rel_yxyx\",\n",
    "    y_true={\n",
    "        \"boxes\": np.array([first_images_unprepared[\"objects\"][\"bbox\"]]),\n",
    "        \"labels\": np.array([first_images_unprepared[\"objects\"][\"label\"]]),\n",
    "    },\n",
    "    scale=4,\n",
    "    class_mapping=CLASSES,\n",
    ")\n",
    "\n",
    "first_images_prepared = next(iter(train_ds_prepared.unbatch().take(1)))\n",
    "\n",
    "plot_bounding_box_gallery(\n",
    "    np.array([first_images_prepared[\"images\"]]),\n",
    "    bounding_box_format=\"yxyx\",\n",
    "    y_true={\n",
    "        \"boxes\": np.array([first_images_prepared[\"bounding_boxes\"][\"boxes\"]]),\n",
    "        \"labels\": np.array([first_images_prepared[\"bounding_boxes\"][\"labels\"]]),\n",
    "    },\n",
    "    scale=4,\n",
    "    class_mapping=CLASSES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "##### Fine-tuning the RetinaNet object detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import keras_hub\n",
    "\n",
    "model = keras_hub.models.ImageObjectDetector.from_preset(\n",
    "    \"retinanet_resnet50_fpn_coco\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model_with_random_head = keras_hub.models.ImageObjectDetector.from_preset(\n",
    "    \"retinanet_resnet50_fpn_coco\",\n",
    "    num_classes=len(CLASSES),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def split_labels(x):\n",
    "    return (\n",
    "        x[\"images\"],\n",
    "        {\n",
    "            \"boxes\": x[\"bounding_boxes\"][\"boxes\"],\n",
    "            \"classes\": x[\"bounding_boxes\"][\"labels\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "train_ds_prepared = train_ds_prepared.map(split_labels)\n",
    "eval_ds_prepared = eval_ds_prepared.map(split_labels)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"pascal_voc_detection.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "    )\n",
    "]\n",
    "history = model.fit(\n",
    "    train_ds_prepared,\n",
    "    validation_data=eval_ds_prepared,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "##### Metrics, evaluation, and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = keras.models.load_model(\"pascal_voc_detection.keras\")\n",
    "images, gt_boxes = next(iter(eval_ds_prepared))\n",
    "predictions = model.predict(images)\n",
    "\n",
    "plot_bounding_box_gallery(\n",
    "    images,\n",
    "    bounding_box_format=BBOX_FORMAT,\n",
    "    y_true={\n",
    "        \"boxes\": gt_boxes[\"boxes\"],\n",
    "        \"labels\": gt_boxes[\"classes\"],\n",
    "    },\n",
    "    y_pred={\n",
    "        \"boxes\": predictions[\"boxes\"],\n",
    "        \"labels\": predictions[\"classes\"],\n",
    "    },\n",
    "    scale=8,\n",
    "    class_mapping=CLASSES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Chapter summary"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "chapter12_object-detection",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}